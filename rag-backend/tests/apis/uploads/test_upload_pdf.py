import uuid

import pytest
from httpx import AsyncClient
from pytest_mock import MockerFixture

import directories


@pytest.mark.asyncio
async def test_upload_pdf(client: AsyncClient, mocker: MockerFixture) -> None:
    mocker.patch(
        target="tempfile.mktemp",
        return_value="/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
    )

    with open(directories.pdf_file, "rb") as pdf_file:
        pdf_binary = pdf_file.read()

        response = await client.post(
            "/v1/uploads/pdf",
            files={
                "files": ("nrms.pdf", pdf_binary, "application/pdf"),
            },
        )

        assert response.status_code == 200
        assert response.json() == [
            {
                "page_content": "Neural News Recommendation with Multi-Head Self-Attention\nChuhan Wu1, Fangzhao Wu2, Suyu Ge1, Tao Qi1, Yongfeng Huang1,and Xing Xie2\n1Department of Electronic Engineering, Tsinghua University, Beijing 100084, China\n2Microsoft Research Asia, Beijing 100080, China\n{wu-ch19, gsy17, qit16, yfhuang }@mails.tsinghua.edu.cn ,\n{fangzwu, xing.xie }@microsoft.com\nAbstract\nNews recommendation can help users ï¬nd in-\nterested news and alleviate information over-\nload. Precisely modeling news and users is\ncritical for news recommendation, and cap-\nturing the contexts of words and news is im-\nportant to learn news and user representations.\nIn this paper, we propose a neural news rec-\nommendation approach with multi-head self-\nattention (NRMS). The core of our approach is\na news encoder and a user encoder. In the news\nencoder, we use multi-head self-attentions to\nlearn news representations from news titles by\nmodeling the interactions between words. In\nthe user encoder, we learn representations of\nusers from their browsed news and use multi-\nhead self-attention to capture the relatedness\nbetween the news. Besides, we apply addi-\ntive attention to learn more informative news\nand user representations by selecting impor-\ntant words and news. Experiments on a real-\nworld dataset validate the effectiveness and ef-\nï¬ciency of our approach.\n1 Introduction\nOnline news platforms such as Google News1and\nMSN News2have attracted many users to read\nnews online (Das et al., 2007). Massive news ar-\nticles are generated everyday and it is impossible\nfor users to read all news to ï¬nd their interested\ncontent (Phelan et al., 2011). Thus, personalized\nnews recommendation is very important for online\nnews platforms to target user interests and allevi-\nate information overload (IJntema et al., 2010).\nLearning accurate representations of news and\nusers are two core tasks in news recommenda-\ntion (Okura et al., 2017). Several deep learn-\ning based methods have been proposed for these\ntasks ( ?Kumar et al., 2017; Khattar et al., 2018;\n1https://news.google.com/\n2https://www.msn.com/en-us/news\nRockets trade Carter-\nWilliams to the Bulls\nBest NBA \nMoments in 2018James Harden's incredible heroics lift Rockets over Warriors in overtimeWeather forecast This Week\nFigure 1: Several news browsed by an example user.\nOrange and green dashed lines represent the interac-\ntions between words and news respectively.\nWu et al., 2019b,c,a; Zhu et al., 2019; An et al.,\n2019). For example, Okura et al. (2017) proposed\nto learn news representations from news bodies via\nauto-encoders, and learn representations of users\nfrom their browsed news via GRU. However, GRU\nis quite time-consuming, and their method cannot\ncapture the contexts of words. Wang et al. (2018)\nproposed to learn news representations from news\ntitles via a knowledge-aware convolutional neural\nnetwork (CNN), and learn representations of users\nbased on the similarities between candidate news\nand their browsed news. However, CNN can-\nnot capture the long-distance contexts of words,\nand their method cannot model the relatedness be-\ntween browsed news.\nOur work is motivated by several observations.\nFirst, the interactions between words in news ti-\ntle are important for understanding the news. For\nexample, in Fig. 1, the word â€œRocketsâ€ has strong\nrelatedness with â€œBullsâ€. Besides, a word may in-\nteract with multiple words, e.g., â€œRocketsâ€ also\nhas semantic interactions with â€œtradeâ€. Second,\ndifferent news articles browsed by the same user\nmay also have relatedness. For example, in Fig. 1\nthe second news is related to the ï¬rst and the third\nnews. Third, different words may have different\nimportance in representing news. In Fig. 1, the\nword â€œNBAâ€ is more informative than â€œ2018â€. Be-\nsides, different news articles browsed by the same\nuser may also have different importance in repre-",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 0,
                },
                "type": "Document",
            },
            {
                "page_content": "senting this user. For example, the ï¬rst three news\narticles are more informative than the last one.\nIn this paper, we propose a neural news\nrecommendation approach with multi-head self-\nattention (NRMS). The core of our approach is a\nnews encoder and a user encoder. In the news en-\ncoder, we learn news representations from news ti-\ntles by using multi-head self-attention to model the\ninteractions between words. In the user encoder,\nwe learn representations of users from their brows-\ning by using multi-head self-attention to capture\ntheir relatedness. Besides, we apply additive at-\ntentions to both news and user encoders to select\nimportant words and news to learn more informa-\ntive news and user representations. Extensive ex-\nperiments on a real-world dataset show that our\napproach can effectively and efï¬ciently improve\nthe performance of news recommendation.\n2 Our Approach\nOur NRMS approach for news recommendation is\nshown in Fig. 2. It contains three modules, i.e.,\nnews encoder ,user encoder andclick predictor .\n2.1 News Encoder\nThenews encoder module is used to learn news\nrepresentations from news titles. It contains three\nlayers. The ï¬rst one is word embedding, which\nis used to convert a news title from a sequence of\nwords into a sequence of low-dimensional embed-\nding vectors. Denote a news title with Mwords\nas[w1,w2,...,w M]. Through this layer it is con-\nverted into a vector sequence [e1,e2,...,eM].\nThe second layer is a word-level multi-head\nself-attention network (Vaswani et al., 2017; Wu\net al., 2018). The interactions between words are\nimportant for learning news representations. For\nexample, in the news title â€œRockets Ends 2018\nwith a Winâ€, the interaction between â€œRocketsâ€\nand â€œWinâ€ is useful for understanding this news,\nand such long-distance interactions usually can-\nnot be captured by CNN. In addition, a word may\ninteract with multiple words in the same news.\nFor instance, in above example the word â€œRock-\netsâ€ has interactions with both â€œEndsâ€ and â€œWinâ€.\nThus, we propose to use multi-head self-attention\nto learn contextual representations of words by\ncapturing their interactions. The representation of\ntheithword learned by the kthattention head is\nâ€¦\nâ€¦â€¦â€¦\nâ€¦\nâ€¦\nNews \nEncoderğ’“ğ’“\nğ·ğ·1 ğ·ğ·ğ‘ğ‘ ğ·ğ·ğ‘ğ‘\nDotï¿½ğ’šğ’š\nClick \nProbability\nCandidate News Browsed NewsWord Embeddingğ’†ğ’†1ğ’†ğ’†2ğ’†ğ’†ğ‘€ğ‘€\nğ‘¤ğ‘¤1ğ‘¤ğ‘¤2ğ‘¤ğ‘¤ğ‘€ğ‘€ğ›¼ğ›¼1ğ‘¤ğ‘¤ğ›¼ğ›¼2ğ‘¤ğ‘¤ğ›¼ğ›¼ğ‘€ğ‘€ğ‘¤ğ‘¤ğ’“ğ’“\nğ’’ğ’’ğ‘¤ğ‘¤\nNews \nEncoderğ’†ğ’†1ğ’†ğ’†2ğ’†ğ’†ğ‘€ğ‘€â€¦\nâ€¦â€¦â€¦\nâ€¦\nâ€¦â€¦\nâ€¦â€¦â€¦\nâ€¦\nâ€¦ğ’‰ğ’‰ğ‘€ğ‘€ğ‘¤ğ‘¤ğ’‰ğ’‰1ğ‘¤ğ‘¤ğ’‰ğ’‰2ğ‘¤ğ‘¤\nâ€¦\nâ€¦â€¦â€¦\nâ€¦\nâ€¦â€¦\nâ€¦â€¦â€¦\nâ€¦\nâ€¦â€¦\nâ€¦â€¦â€¦\nâ€¦\nâ€¦\nğ’“ğ’“1ğ’“ğ’“ğ‘ğ‘\nğ’“ğ’“2\nNews \nEncoderğ’“ğ’“1 ğ’“ğ’“ğ‘ğ‘ğ’“ğ’“2ğ’‰ğ’‰ğ‘ğ‘ğ‘›ğ‘›ğ’‰ğ’‰1ğ‘›ğ‘›ğ’‰ğ’‰2ğ‘›ğ‘›ğ›¼ğ›¼1ğ‘›ğ‘›ğ›¼ğ›¼2ğ‘›ğ‘›ğ›¼ğ›¼ğ‘€ğ‘€ğ‘›ğ‘›ğ’–ğ’–\nğ’’ğ’’ğ‘›ğ‘›Figure 2: The framework of our NRMS approach.\ncomputed as:\nÎ±k\ni,j=exp(eT\niQw\nkej)âˆ‘M\nm=1exp(eT\niQw\nkem), (1)\nhw\ni,k=Vw\nk(Mâˆ‘\nj=1Î±k\ni,jej), (2)\nwhere Qw\nkandVw\nkare the projection parameters\nin thekthself-attention head, and Î±k\ni,jindicates the\nrelative importance of the interaction between the\nithandjthwords. The multi-head representation\nhw\niof theithword is the concatenation of the rep-\nresentations produced by hseparate self-attention\nheads, i.e., hw\ni= [hw\ni,1;hw\ni,2;...;hw\ni,h].\nThe third layer is an additive word attention net-\nwork. Different words in the same news may have\ndifferent importance in representing this news. For\nexample, in the second news of Fig. 1, the word\nâ€œNFLâ€ is more informative than â€œTodayâ€ for un-\nderstanding this news. Thus, we propose to use\nattention mechanism to select important words in\nnews titles for learning more informative news\nrepresentations. The attention weight Î±w\niof the\ni-th word in a news title is computed as:\naw\ni=qT\nwtanh( VwÃ—hw\ni+vw), (3)\nÎ±w\ni=exp(aw\ni)âˆ‘M\nj=1exp(aw\nj), (4)\nwhere Vwandvware projection parameters, and\nqwis the query vector. The ï¬nal representation of\na news is the weighted summation of the contex-\ntual word representations, formulated as:\nr=Mâˆ‘\ni=1Î±w\nihw\ni. (5)",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 1,
                },
                "type": "Document",
            },
            {
                "page_content": "2.2 User Encoder\nTheuser encoder module is used to learn the rep-\nresentations of users from their browsed news.\nIt contains two layers. The ï¬rst one is a news-\nlevel multi-head self-attention network. Usually,\nnews articles browsed by the same user may have\nsome relatedness. For example, in Fig. 1, the\nï¬rst two news articles are related. In addition, a\nnews article may interact with multiple news arti-\ncles browsed by the same user. Thus, we propose\nto apply multi-head self-attention to enhance the\nrepresentations of news by capturing their interac-\ntions. The representation of the ithnews learned\nby thekthattention head is formulated as follows:\nÎ²k\ni,j=exp(rT\niQn\nkrj)âˆ‘M\nm=1exp(rT\niQn\nkrm), (6)\nhn\ni,k=Vn\nk(Mâˆ‘\nj=1Î²k\ni,jrj), (7)\nwhere Qn\nkandVn\nkare parameters of the kthnews\nself-attention head, and Î²k\ni,jrepresents the relative\nimportance of the interaction between the jthand\nthekthnews. The multi-head representation of\ntheithnews is the concatenation of the represen-\ntations output by hseparate self-attention heads,\ni.e.,hn\ni= [hn\ni,1;hn\ni,2;...;hn\ni,h].\nThe second layer is an additive news attention\nnetwork. Different news may have different in-\nformativeness in representing users. For example,\nin Fig. 1 the ï¬rst news is more informative than\nthe fourth news in modeling user interest, since\nthe latter one is usually browsed by massive users.\nThus, we propose to apply the additive attention\nmechanism to select important news to learn more\ninformative user representations. The attention\nweight of the ithnews is computed as:\nan\ni=qT\nntanh( VnÃ—hn\ni+vn), (8)\nÎ±n\ni=exp(an\ni)âˆ‘N\nj=1exp(an\nj), (9)\nwhere Vn,vnandqnare parameters in the at-\ntention network, and Nis the number of the\nbrowsed news. The ï¬nal user representation is the\nweighted summation of the representations of the\nnews browsed by this user, which is formulated as:\nu=Nâˆ‘\ni=1Î±n\nihn\ni. (10)2.3 Click Predictor\nTheclick predictor module is used to predict the\nprobability of a user clicking a candidate news.\nDenote the representation of a candidate news Dc\nasrc. Following (Okura et al., 2017), the click\nprobability score Ë†yis computed by the inner prod-\nuct of the user representation vector and the news\nrepresentation vector, i.e., Ë†y=uTrc. We also\nexplored other kinds of scoring methods such as\nperception, but dot product shows the best perfor-\nmance and efï¬ciency.\n2.4 Model Training\nMotivated by (Huang et al., 2013), we use nega-\ntive sampling techniques for model training. For\neach news browsed by a user (regarded as a posi-\ntive sample), we randomly sample Knews which\nare shown in the same impression but not clicked\nby the user (regarded as negative samples). We\nshufï¬‚e the orders of these news to avoid possi-\nble positional biases. Denote the click probability\nscore of the positive and the Knegative news as\nË†y+and[Ë†yâˆ’\n1,Ë†yâˆ’\n2,...,Ë†yâˆ’\nK]respectively. These scores\nare normalized by the softmax function to com-\npute the posterior click probability of a positive\nsample as follows:\npi=exp(Ë†y+\ni)\nexp(Ë†y+\ni) +âˆ‘K\nj=1exp(Ë†yâˆ’\ni,j). (11)\nWe re-formulate the news click probability predic-\ntion problem as a pseudo (K+ 1) -way classiï¬ca-\ntion task, and the loss function for model training\nis the negative log-likelihood of all positive sam-\nplesS, which is formulated as follows:\nL=âˆ’âˆ‘\niâˆˆSlog(pi). (12)\n3 Experiments\n3.1 Datasets and Experimental Settings\nWe conducted experiments on a real-world news\nrecommendation dataset collected from MSN\nNews3logs in one month (Dec. 13, 2018 to Jan.\n12, 2019). The detailed statistics are shown in Ta-\nble 1. The logs in the last week were used for test,\nand the rest were used for training. We randomly\nsampled 10% of training data for validation.\n3https://www.msn.com/en-us/news",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 2,
                },
                "type": "Document",
            },
            {
                "page_content": "# users 10,000 avg. # words per title 11.29\n# news 42,255 # positive samples 489,644\n# impressions 445,230 # negative samples 6,651,940\nTable 1: Statistics of our dataset.\nIn our experiments, the word embeddings are\n300-dimensional and initialized by the Glove em-\nbedding (Pennington et al., 2014). The self-\nattention networks have 16 heads, and the out-\nput of each head is 16-dimensional. The dimen-\nsion of the additive attention query vectors is 200.\nFollowing (Wu et al., 2019b), the negative sam-\npling ratioKis 4. Adam (Kingma and Ba, 2014)\nis used for model optimization. We apply 20%\ndropout to the word embeddings to mitigate over-\nï¬tting. The batch size is 64. These hyperparam-\neters are tuned on validation set. We conducted\nexperiments on a machine with Xeon E5-2620 v4\nCPUs and a GTX1080Ti GPU. We independently\nrepeated each experiment 10 times and reported\naverage results in terms of AUC, MRR, nDCG@5\nand nDCG@10.\n3.2 Performance Evaluation\nWe evaluate the performance of our approach by\ncomparing it with several baseline methods, in-\ncluding: (1) LibFM (Rendle, 2012), a matrix\nfactorization based recommendation method; (2)\nDSSM (Huang et al., 2013), deep structured se-\nmantic model; (3) Wide&Deep (Cheng et al.,\n2016), a popular neural recommendation method;\n(4)DeepFM (Guo et al., 2017), another popular\nneural recommendation method; (5) DFM (Lian\net al., 2018), deep fusion model for news recom-\nmendation; (6) DKN (Wang et al., 2018), deep\nknowledge-aware network for news recommenda-\ntion; (7) Conv3D (Khattar et al., 2018), a neural\nnews recommendation method with 3-D CNNs to\nlearn user representations; (8) GRU (Okura et al.,\n2017), a neural news recommendation method us-\ning GRU to learn user representations; (9) NRMS ,\nour approach. In methods (1) and (3-5), we use\none-hot encoded user ID, news ID and the TF-IDF\nfeatures extracted from news titles as the model\ninput. In methods (6-9), we all use news titles for\nfair comparison. The results of these methods are\nsummarized in Table 2.\nWe have several observations from Table 2.\nFirst, neural recommendation methods such as\nDSSM andNRMS outperform traditional recom-\nmendation methods such as LibFM on news rec-Methods AUC MRR nDCG@5 nDCG@10\nLibFM 0.5661 0.2414 0.2689 0.3552\nDSSM 0.5949 0.2675 0.2881 0.3800\nWide&Deep 0.5812 0.2546 0.2765 0.3674\nDeepFM 0.5830 0.2570 0.2802 0.3707\nDFM 0.5861 0.2609 0.2844 0.3742\nDKN 0.6032 0.2744 0.2967 0.3873\nConv3D 0.6051 0.2765 0.2987 0.3904\nGRU 0.6102 0.2811 0.3035 0.3952\nNRMS* 0.6275 0.2985 0.3217 0.4139\nTable 2: The results of different methods. *The im-\nprovement is signiï¬cant at p<0.01.\nommendation. This may be because neural net-\nworks can learn better representations of news and\nusers than matrix factorization methods. Thus, it\nmay be more appropriate to learn news and user\nrepresentations via neural networks rather than\ncraft them manually. Second, among the deep\nlearning based methods, the methods which ex-\nploit the relatedness between news (e.g., Conv3D ,\nGRU andNRMS ) can outperform other methods.\nThis may be because the news browsed by the\nsame user usually have relatedness, and capturing\nthe news relatedness is useful for understanding\nthese news and modeling user interests. Third, our\napproach performs better than all baseline meth-\nods. This is because our approach can capture\nthe interactions between both words and news via\nmulti-head self-attention to enhance representa-\ntion learning of news and users. Besides, our ap-\nproach employs additive attention to select impor-\ntant words and news for learning informative news\nand user representations. These results validate\nthe effectiveness of our approach.\nWe also conducted experiments to compare the\ntime efï¬ciency of our approach with several pop-\nular news recommendation methods. The results\nare shown in Table 3. From the results, we ï¬nd\nour approach has a smaller parameter size and a\nlower time complexity in learning news and user\nrepresentations than existing news recommenda-\ntion methods. In addition, different from DKN ,\nour approach does not need to memorize the news\nbrowsing histories of users when computing the\nclick probability scores. In addition, since our\napproach can be further accelerated by comput-\ning the hidden representations of different atten-\ntion heads in parallel, our approach is more suit-\nable for being deployed in large-scale news rec-\nommendation scenarios. These results validate the\nefï¬ciency of our approach.",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 3,
                },
                "type": "Document",
            },
            {
                "page_content": "Method # Parameters News Encoding Time User Encoding Time\nDKN 681 K 46.4 s 10.1 min\nConv3D 575 K 29.8 min 16.7 min\nGRU 541 K 59.8 s 148.0 min\nNRMS 530 K 39.7 s 6.7 min\nTable 3: The number of parameters in NRMS and base-\nline methods, and their time in encoding 1 million news\nand 1 million users. *Word embeddings are excluded.\n(a) Attention mechanisms at different levels.\n(b) Attention mechanisms of different kinds.\nFigure 3: Effectiveness of different attention networks.\n3.3 Effectiveness of Attention Mechanism\nNext we explore the effectiveness of attentions\nin our approach. First, we verify the word- and\nnews-level attentions. The results are shown in\nFig. 3(a). We ï¬nd the word-level attention is very\nuseful. This is because modeling the interactions\nbetween words and selecting important words can\nhelp learn informative news representations. Be-\nsides, the news-level attention is also useful. This\nis because capturing the relatedness of news and\nselecting important news can beneï¬t the learn-\ning of user representations. Moreover, combining\nboth word- and news-level attentions can further\nimprove the performance of our approach.\nWe also study the inï¬‚uence of additive and self-\nattentions on our approach. The results are shown\nin Fig. 3(b). From these results, we ï¬nd the self-\nattentions are very useful. This is because theinteractions between words and news are impor-\ntant for understanding news and modeling users.\nIn addition, the additive attentions are also help-\nful. This is because different words and news may\nusually have different importance in representing\nnews and users. Thus, selecting important words\nand news can help learn more informative news\nand user representations. Combining both addi-\ntive and self-attention can further improve our ap-\nproach. Thus, these results validate the effective-\nness of the attention mechanism in our approach.\n4 Conclusion and Future Work\nIn this paper we propose a neural news recommen-\ndation approach with multi-head self-attention.\nThe core of our approach is a news encoder and\na user encoder. In both encoders we apply multi-\nhead self-attentions to learn contextual word and\nnews representations by modeling the interactions\nbetween words and news. In addition, we use\nadditive attentions to select important words and\nnews to learn more informative news and user rep-\nresentations. Extensive experiments validate the\neffectiveness and efï¬ciency of our approach.\nIn our future work, we will try to improve\nour approach in the following potential directions.\nFirst, in our framework we do not consider the po-\nsitional information of words and news, but they\nmay be useful for learning more accurate news and\nuser representations. We will explore position en-\ncoding techniques to incorporate the word position\nand the time-stamps of news clicks to further en-\nhance our approach. Second, we will explore how\nto effectively incorporate multiple kinds of news\ninformation in our framework, especially long se-\nquences such as news body, which may challenge\nthe efï¬ciency of typical self-attention networks.\nAcknowledgments\nThe authors would like to thank Microsoft News\nfor providing technical support and data in the\nexperiments, and Jiun-Hung Chen (Microsoft\nNews) and Ying Qiao (Microsoft News) for their\nsupport and discussions. This work was sup-\nported by the National Key Research and De-\nvelopment Program of China under Grant num-\nber 2018YFC1604002, the National Natural Sci-\nence Foundation of China under Grant numbers\nU1836204, U1705261, U1636113, U1536201,\nand U1536207.",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 4,
                },
                "type": "Document",
            },
            {
                "page_content": "References\nMingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang,\nZheng Liu, and Xing Xie. 2019. Neural news rec-\nommendation with long-and short-term user repre-\nsentations. In ACL, pages 336â€“345.\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal\nShaked, Tushar Chandra, Hrishi Aradhye, Glen An-\nderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.\n2016. Wide & deep learning for recommender sys-\ntems. In DLRS , pages 7â€“10.\nAbhinandan S Das, Mayur Datar, Ashutosh Garg, and\nShyam Rajaram. 2007. Google news personal-\nization: scalable online collaborative ï¬ltering. In\nWWW , pages 271â€“280. ACM.\nHuifeng Guo, Ruiming Tang, Yunming Ye, Zhen-\nguo Li, and Xiuqiang He. 2017. Deepfm: a\nfactorization-machine based neural network for ctr\nprediction. In AAAI , pages 1725â€“1731. AAAI Press.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In CIKM , pages 2333â€“2338.\nACM.\nWouter IJntema, Frank Goossen, Flavius Frasincar,\nand Frederik Hogenboom. 2010. Ontology-based\nnews recommendation. In Proceedings of the 2010\nEDBT/ICDT Workshops , page 16.\nDhruv Khattar, Vaibhav Kumar, Vasudeva Varma, and\nManish Gupta. 2018. Weave&rec: A word embed-\nding based 3-d convolutional network for news rec-\nommendation. In CIKM , pages 1855â€“1858.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 .\nVaibhav Kumar, Dhruv Khattar, Shashank Gupta, Man-\nish Gupta, and Vasudeva Varma. 2017. Deep neu-\nral architecture for news recommendation. In CLEF\n(Working Notes) .\nJianxun Lian, Fuzheng Zhang, Xing Xie, and\nGuangzhong Sun. 2018. Towards better represen-\ntation learning for personalized news recommenda-\ntion: a multi-channel deep fusion approach. In IJ-\nCAI, pages 3805â€“3811.Shumpei Okura, Yukihiro Tagami, Shingo Ono, and\nAkira Tajima. 2017. Embedding-based news rec-\nommendation for millions of users. In KDD , pages\n1933â€“1942. ACM.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP , pages 1532â€“1543.\nOwen Phelan, Kevin McCarthy, Mike Bennett, and\nBarry Smyth. 2011. Terms of a feather: Content-\nbased news recommendation and discovery using\ntwitter. In ECIR , pages 448â€“459. Springer.\nSteffen Rendle. 2012. Factorization machines with\nlibfm. TIST , 3(3):57.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS , pages 5998â€“6008.\nHongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi\nGuo. 2018. Dkn: Deep knowledge-aware network\nfor news recommendation. In WWW , pages 1835â€“\n1844.\nChuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang\nHuang, Yongfeng Huang, and Xing Xie. 2019a.\nNeural news recommendation with attentive multi-\nview learning. In IJCAI , pages 3863â€“3869.\nChuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang\nHuang, Yongfeng Huang, and Xing Xie. 2019b.\nNpa: Neural news recommendation with personal-\nized attention. In KDD , pages 2576â€“2584. ACM.\nChuhan Wu, Fangzhao Wu, Mingxiao An, Yongfeng\nHuang, and Xing Xie. 2019c. Neural news recom-\nmendation with topic-aware news representation. In\nACL, pages 1154â€“1159.\nChuhan Wu, Fangzhao Wu, Junxin Liu, Sixing Wu,\nYongfeng Huang, and Xing Xie. 2018. Detect-\ning tweets mentioning drug name and adverse drug\nreaction with hierarchical tweet representation and\nmulti-head self-attention. In SMM4H , pages 34â€“37.\nQiannan Zhu, Xiaofei Zhou, Zeliang Song, Jianlong\nTan, and Li Guo. 2019. Dan: Deep attention neural\nnetwork for news recommendation. In AAAI , vol-\nume 33, pages 5973â€“5980.",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 5,
                },
                "type": "Document",
            },
        ]
