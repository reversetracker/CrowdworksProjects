import uuid

import pytest
from httpx import AsyncClient
from pytest_mock import MockerFixture

import directories


@pytest.mark.asyncio
async def test_upload_pdf(client: AsyncClient, mocker: MockerFixture) -> None:
    mocker.patch(
        target="tempfile.mktemp",
        return_value="/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
    )

    with open(directories.pdf_file, "rb") as pdf_file:
        pdf_binary = pdf_file.read()

        response = await client.post(
            "/v1/uploads/pdf",
            files={
                "files": ("nrms.pdf", pdf_binary, "application/pdf"),
            },
        )

        assert response.status_code == 200
        assert response.json() == [
            {
                "page_content": "Neural News Recommendation with Multi-Head Self-Attention\nChuhan Wu1, Fangzhao Wu2, Suyu Ge1, Tao Qi1, Yongfeng Huang1,and Xing Xie2\n1Department of Electronic Engineering, Tsinghua University, Beijing 100084, China\n2Microsoft Research Asia, Beijing 100080, China\n{wu-ch19, gsy17, qit16, yfhuang }@mails.tsinghua.edu.cn ,\n{fangzwu, xing.xie }@microsoft.com\nAbstract\nNews recommendation can help users ﬁnd in-\nterested news and alleviate information over-\nload. Precisely modeling news and users is\ncritical for news recommendation, and cap-\nturing the contexts of words and news is im-\nportant to learn news and user representations.\nIn this paper, we propose a neural news rec-\nommendation approach with multi-head self-\nattention (NRMS). The core of our approach is\na news encoder and a user encoder. In the news\nencoder, we use multi-head self-attentions to\nlearn news representations from news titles by\nmodeling the interactions between words. In\nthe user encoder, we learn representations of\nusers from their browsed news and use multi-\nhead self-attention to capture the relatedness\nbetween the news. Besides, we apply addi-\ntive attention to learn more informative news\nand user representations by selecting impor-\ntant words and news. Experiments on a real-\nworld dataset validate the effectiveness and ef-\nﬁciency of our approach.\n1 Introduction\nOnline news platforms such as Google News1and\nMSN News2have attracted many users to read\nnews online (Das et al., 2007). Massive news ar-\nticles are generated everyday and it is impossible\nfor users to read all news to ﬁnd their interested\ncontent (Phelan et al., 2011). Thus, personalized\nnews recommendation is very important for online\nnews platforms to target user interests and allevi-\nate information overload (IJntema et al., 2010).\nLearning accurate representations of news and\nusers are two core tasks in news recommenda-\ntion (Okura et al., 2017). Several deep learn-\ning based methods have been proposed for these\ntasks ( ?Kumar et al., 2017; Khattar et al., 2018;\n1https://news.google.com/\n2https://www.msn.com/en-us/news\nRockets trade Carter-\nWilliams to the Bulls\nBest NBA \nMoments in 2018James Harden's incredible heroics lift Rockets over Warriors in overtimeWeather forecast This Week\nFigure 1: Several news browsed by an example user.\nOrange and green dashed lines represent the interac-\ntions between words and news respectively.\nWu et al., 2019b,c,a; Zhu et al., 2019; An et al.,\n2019). For example, Okura et al. (2017) proposed\nto learn news representations from news bodies via\nauto-encoders, and learn representations of users\nfrom their browsed news via GRU. However, GRU\nis quite time-consuming, and their method cannot\ncapture the contexts of words. Wang et al. (2018)\nproposed to learn news representations from news\ntitles via a knowledge-aware convolutional neural\nnetwork (CNN), and learn representations of users\nbased on the similarities between candidate news\nand their browsed news. However, CNN can-\nnot capture the long-distance contexts of words,\nand their method cannot model the relatedness be-\ntween browsed news.\nOur work is motivated by several observations.\nFirst, the interactions between words in news ti-\ntle are important for understanding the news. For\nexample, in Fig. 1, the word “Rockets” has strong\nrelatedness with “Bulls”. Besides, a word may in-\nteract with multiple words, e.g., “Rockets” also\nhas semantic interactions with “trade”. Second,\ndifferent news articles browsed by the same user\nmay also have relatedness. For example, in Fig. 1\nthe second news is related to the ﬁrst and the third\nnews. Third, different words may have different\nimportance in representing news. In Fig. 1, the\nword “NBA” is more informative than “2018”. Be-\nsides, different news articles browsed by the same\nuser may also have different importance in repre-",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 0,
                },
                "type": "Document",
            },
            {
                "page_content": "senting this user. For example, the ﬁrst three news\narticles are more informative than the last one.\nIn this paper, we propose a neural news\nrecommendation approach with multi-head self-\nattention (NRMS). The core of our approach is a\nnews encoder and a user encoder. In the news en-\ncoder, we learn news representations from news ti-\ntles by using multi-head self-attention to model the\ninteractions between words. In the user encoder,\nwe learn representations of users from their brows-\ning by using multi-head self-attention to capture\ntheir relatedness. Besides, we apply additive at-\ntentions to both news and user encoders to select\nimportant words and news to learn more informa-\ntive news and user representations. Extensive ex-\nperiments on a real-world dataset show that our\napproach can effectively and efﬁciently improve\nthe performance of news recommendation.\n2 Our Approach\nOur NRMS approach for news recommendation is\nshown in Fig. 2. It contains three modules, i.e.,\nnews encoder ,user encoder andclick predictor .\n2.1 News Encoder\nThenews encoder module is used to learn news\nrepresentations from news titles. It contains three\nlayers. The ﬁrst one is word embedding, which\nis used to convert a news title from a sequence of\nwords into a sequence of low-dimensional embed-\nding vectors. Denote a news title with Mwords\nas[w1,w2,...,w M]. Through this layer it is con-\nverted into a vector sequence [e1,e2,...,eM].\nThe second layer is a word-level multi-head\nself-attention network (Vaswani et al., 2017; Wu\net al., 2018). The interactions between words are\nimportant for learning news representations. For\nexample, in the news title “Rockets Ends 2018\nwith a Win”, the interaction between “Rockets”\nand “Win” is useful for understanding this news,\nand such long-distance interactions usually can-\nnot be captured by CNN. In addition, a word may\ninteract with multiple words in the same news.\nFor instance, in above example the word “Rock-\nets” has interactions with both “Ends” and “Win”.\nThus, we propose to use multi-head self-attention\nto learn contextual representations of words by\ncapturing their interactions. The representation of\ntheithword learned by the kthattention head is\n…\n………\n…\n…\nNews \nEncoder𝒓𝒓\n𝐷𝐷1 𝐷𝐷𝑁𝑁 𝐷𝐷𝑐𝑐\nDot�𝒚𝒚\nClick \nProbability\nCandidate News Browsed NewsWord Embedding𝒆𝒆1𝒆𝒆2𝒆𝒆𝑀𝑀\n𝑤𝑤1𝑤𝑤2𝑤𝑤𝑀𝑀𝛼𝛼1𝑤𝑤𝛼𝛼2𝑤𝑤𝛼𝛼𝑀𝑀𝑤𝑤𝒓𝒓\n𝒒𝒒𝑤𝑤\nNews \nEncoder𝒆𝒆1𝒆𝒆2𝒆𝒆𝑀𝑀…\n………\n…\n……\n………\n…\n…𝒉𝒉𝑀𝑀𝑤𝑤𝒉𝒉1𝑤𝑤𝒉𝒉2𝑤𝑤\n…\n………\n…\n……\n………\n…\n……\n………\n…\n…\n𝒓𝒓1𝒓𝒓𝑁𝑁\n𝒓𝒓2\nNews \nEncoder𝒓𝒓1 𝒓𝒓𝑁𝑁𝒓𝒓2𝒉𝒉𝑁𝑁𝑛𝑛𝒉𝒉1𝑛𝑛𝒉𝒉2𝑛𝑛𝛼𝛼1𝑛𝑛𝛼𝛼2𝑛𝑛𝛼𝛼𝑀𝑀𝑛𝑛𝒖𝒖\n𝒒𝒒𝑛𝑛Figure 2: The framework of our NRMS approach.\ncomputed as:\nαk\ni,j=exp(eT\niQw\nkej)∑M\nm=1exp(eT\niQw\nkem), (1)\nhw\ni,k=Vw\nk(M∑\nj=1αk\ni,jej), (2)\nwhere Qw\nkandVw\nkare the projection parameters\nin thekthself-attention head, and αk\ni,jindicates the\nrelative importance of the interaction between the\nithandjthwords. The multi-head representation\nhw\niof theithword is the concatenation of the rep-\nresentations produced by hseparate self-attention\nheads, i.e., hw\ni= [hw\ni,1;hw\ni,2;...;hw\ni,h].\nThe third layer is an additive word attention net-\nwork. Different words in the same news may have\ndifferent importance in representing this news. For\nexample, in the second news of Fig. 1, the word\n“NFL” is more informative than “Today” for un-\nderstanding this news. Thus, we propose to use\nattention mechanism to select important words in\nnews titles for learning more informative news\nrepresentations. The attention weight αw\niof the\ni-th word in a news title is computed as:\naw\ni=qT\nwtanh( Vw×hw\ni+vw), (3)\nαw\ni=exp(aw\ni)∑M\nj=1exp(aw\nj), (4)\nwhere Vwandvware projection parameters, and\nqwis the query vector. The ﬁnal representation of\na news is the weighted summation of the contex-\ntual word representations, formulated as:\nr=M∑\ni=1αw\nihw\ni. (5)",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 1,
                },
                "type": "Document",
            },
            {
                "page_content": "2.2 User Encoder\nTheuser encoder module is used to learn the rep-\nresentations of users from their browsed news.\nIt contains two layers. The ﬁrst one is a news-\nlevel multi-head self-attention network. Usually,\nnews articles browsed by the same user may have\nsome relatedness. For example, in Fig. 1, the\nﬁrst two news articles are related. In addition, a\nnews article may interact with multiple news arti-\ncles browsed by the same user. Thus, we propose\nto apply multi-head self-attention to enhance the\nrepresentations of news by capturing their interac-\ntions. The representation of the ithnews learned\nby thekthattention head is formulated as follows:\nβk\ni,j=exp(rT\niQn\nkrj)∑M\nm=1exp(rT\niQn\nkrm), (6)\nhn\ni,k=Vn\nk(M∑\nj=1βk\ni,jrj), (7)\nwhere Qn\nkandVn\nkare parameters of the kthnews\nself-attention head, and βk\ni,jrepresents the relative\nimportance of the interaction between the jthand\nthekthnews. The multi-head representation of\ntheithnews is the concatenation of the represen-\ntations output by hseparate self-attention heads,\ni.e.,hn\ni= [hn\ni,1;hn\ni,2;...;hn\ni,h].\nThe second layer is an additive news attention\nnetwork. Different news may have different in-\nformativeness in representing users. For example,\nin Fig. 1 the ﬁrst news is more informative than\nthe fourth news in modeling user interest, since\nthe latter one is usually browsed by massive users.\nThus, we propose to apply the additive attention\nmechanism to select important news to learn more\ninformative user representations. The attention\nweight of the ithnews is computed as:\nan\ni=qT\nntanh( Vn×hn\ni+vn), (8)\nαn\ni=exp(an\ni)∑N\nj=1exp(an\nj), (9)\nwhere Vn,vnandqnare parameters in the at-\ntention network, and Nis the number of the\nbrowsed news. The ﬁnal user representation is the\nweighted summation of the representations of the\nnews browsed by this user, which is formulated as:\nu=N∑\ni=1αn\nihn\ni. (10)2.3 Click Predictor\nTheclick predictor module is used to predict the\nprobability of a user clicking a candidate news.\nDenote the representation of a candidate news Dc\nasrc. Following (Okura et al., 2017), the click\nprobability score ˆyis computed by the inner prod-\nuct of the user representation vector and the news\nrepresentation vector, i.e., ˆy=uTrc. We also\nexplored other kinds of scoring methods such as\nperception, but dot product shows the best perfor-\nmance and efﬁciency.\n2.4 Model Training\nMotivated by (Huang et al., 2013), we use nega-\ntive sampling techniques for model training. For\neach news browsed by a user (regarded as a posi-\ntive sample), we randomly sample Knews which\nare shown in the same impression but not clicked\nby the user (regarded as negative samples). We\nshufﬂe the orders of these news to avoid possi-\nble positional biases. Denote the click probability\nscore of the positive and the Knegative news as\nˆy+and[ˆy−\n1,ˆy−\n2,...,ˆy−\nK]respectively. These scores\nare normalized by the softmax function to com-\npute the posterior click probability of a positive\nsample as follows:\npi=exp(ˆy+\ni)\nexp(ˆy+\ni) +∑K\nj=1exp(ˆy−\ni,j). (11)\nWe re-formulate the news click probability predic-\ntion problem as a pseudo (K+ 1) -way classiﬁca-\ntion task, and the loss function for model training\nis the negative log-likelihood of all positive sam-\nplesS, which is formulated as follows:\nL=−∑\ni∈Slog(pi). (12)\n3 Experiments\n3.1 Datasets and Experimental Settings\nWe conducted experiments on a real-world news\nrecommendation dataset collected from MSN\nNews3logs in one month (Dec. 13, 2018 to Jan.\n12, 2019). The detailed statistics are shown in Ta-\nble 1. The logs in the last week were used for test,\nand the rest were used for training. We randomly\nsampled 10% of training data for validation.\n3https://www.msn.com/en-us/news",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 2,
                },
                "type": "Document",
            },
            {
                "page_content": "# users 10,000 avg. # words per title 11.29\n# news 42,255 # positive samples 489,644\n# impressions 445,230 # negative samples 6,651,940\nTable 1: Statistics of our dataset.\nIn our experiments, the word embeddings are\n300-dimensional and initialized by the Glove em-\nbedding (Pennington et al., 2014). The self-\nattention networks have 16 heads, and the out-\nput of each head is 16-dimensional. The dimen-\nsion of the additive attention query vectors is 200.\nFollowing (Wu et al., 2019b), the negative sam-\npling ratioKis 4. Adam (Kingma and Ba, 2014)\nis used for model optimization. We apply 20%\ndropout to the word embeddings to mitigate over-\nﬁtting. The batch size is 64. These hyperparam-\neters are tuned on validation set. We conducted\nexperiments on a machine with Xeon E5-2620 v4\nCPUs and a GTX1080Ti GPU. We independently\nrepeated each experiment 10 times and reported\naverage results in terms of AUC, MRR, nDCG@5\nand nDCG@10.\n3.2 Performance Evaluation\nWe evaluate the performance of our approach by\ncomparing it with several baseline methods, in-\ncluding: (1) LibFM (Rendle, 2012), a matrix\nfactorization based recommendation method; (2)\nDSSM (Huang et al., 2013), deep structured se-\nmantic model; (3) Wide&Deep (Cheng et al.,\n2016), a popular neural recommendation method;\n(4)DeepFM (Guo et al., 2017), another popular\nneural recommendation method; (5) DFM (Lian\net al., 2018), deep fusion model for news recom-\nmendation; (6) DKN (Wang et al., 2018), deep\nknowledge-aware network for news recommenda-\ntion; (7) Conv3D (Khattar et al., 2018), a neural\nnews recommendation method with 3-D CNNs to\nlearn user representations; (8) GRU (Okura et al.,\n2017), a neural news recommendation method us-\ning GRU to learn user representations; (9) NRMS ,\nour approach. In methods (1) and (3-5), we use\none-hot encoded user ID, news ID and the TF-IDF\nfeatures extracted from news titles as the model\ninput. In methods (6-9), we all use news titles for\nfair comparison. The results of these methods are\nsummarized in Table 2.\nWe have several observations from Table 2.\nFirst, neural recommendation methods such as\nDSSM andNRMS outperform traditional recom-\nmendation methods such as LibFM on news rec-Methods AUC MRR nDCG@5 nDCG@10\nLibFM 0.5661 0.2414 0.2689 0.3552\nDSSM 0.5949 0.2675 0.2881 0.3800\nWide&Deep 0.5812 0.2546 0.2765 0.3674\nDeepFM 0.5830 0.2570 0.2802 0.3707\nDFM 0.5861 0.2609 0.2844 0.3742\nDKN 0.6032 0.2744 0.2967 0.3873\nConv3D 0.6051 0.2765 0.2987 0.3904\nGRU 0.6102 0.2811 0.3035 0.3952\nNRMS* 0.6275 0.2985 0.3217 0.4139\nTable 2: The results of different methods. *The im-\nprovement is signiﬁcant at p<0.01.\nommendation. This may be because neural net-\nworks can learn better representations of news and\nusers than matrix factorization methods. Thus, it\nmay be more appropriate to learn news and user\nrepresentations via neural networks rather than\ncraft them manually. Second, among the deep\nlearning based methods, the methods which ex-\nploit the relatedness between news (e.g., Conv3D ,\nGRU andNRMS ) can outperform other methods.\nThis may be because the news browsed by the\nsame user usually have relatedness, and capturing\nthe news relatedness is useful for understanding\nthese news and modeling user interests. Third, our\napproach performs better than all baseline meth-\nods. This is because our approach can capture\nthe interactions between both words and news via\nmulti-head self-attention to enhance representa-\ntion learning of news and users. Besides, our ap-\nproach employs additive attention to select impor-\ntant words and news for learning informative news\nand user representations. These results validate\nthe effectiveness of our approach.\nWe also conducted experiments to compare the\ntime efﬁciency of our approach with several pop-\nular news recommendation methods. The results\nare shown in Table 3. From the results, we ﬁnd\nour approach has a smaller parameter size and a\nlower time complexity in learning news and user\nrepresentations than existing news recommenda-\ntion methods. In addition, different from DKN ,\nour approach does not need to memorize the news\nbrowsing histories of users when computing the\nclick probability scores. In addition, since our\napproach can be further accelerated by comput-\ning the hidden representations of different atten-\ntion heads in parallel, our approach is more suit-\nable for being deployed in large-scale news rec-\nommendation scenarios. These results validate the\nefﬁciency of our approach.",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 3,
                },
                "type": "Document",
            },
            {
                "page_content": "Method # Parameters News Encoding Time User Encoding Time\nDKN 681 K 46.4 s 10.1 min\nConv3D 575 K 29.8 min 16.7 min\nGRU 541 K 59.8 s 148.0 min\nNRMS 530 K 39.7 s 6.7 min\nTable 3: The number of parameters in NRMS and base-\nline methods, and their time in encoding 1 million news\nand 1 million users. *Word embeddings are excluded.\n(a) Attention mechanisms at different levels.\n(b) Attention mechanisms of different kinds.\nFigure 3: Effectiveness of different attention networks.\n3.3 Effectiveness of Attention Mechanism\nNext we explore the effectiveness of attentions\nin our approach. First, we verify the word- and\nnews-level attentions. The results are shown in\nFig. 3(a). We ﬁnd the word-level attention is very\nuseful. This is because modeling the interactions\nbetween words and selecting important words can\nhelp learn informative news representations. Be-\nsides, the news-level attention is also useful. This\nis because capturing the relatedness of news and\nselecting important news can beneﬁt the learn-\ning of user representations. Moreover, combining\nboth word- and news-level attentions can further\nimprove the performance of our approach.\nWe also study the inﬂuence of additive and self-\nattentions on our approach. The results are shown\nin Fig. 3(b). From these results, we ﬁnd the self-\nattentions are very useful. This is because theinteractions between words and news are impor-\ntant for understanding news and modeling users.\nIn addition, the additive attentions are also help-\nful. This is because different words and news may\nusually have different importance in representing\nnews and users. Thus, selecting important words\nand news can help learn more informative news\nand user representations. Combining both addi-\ntive and self-attention can further improve our ap-\nproach. Thus, these results validate the effective-\nness of the attention mechanism in our approach.\n4 Conclusion and Future Work\nIn this paper we propose a neural news recommen-\ndation approach with multi-head self-attention.\nThe core of our approach is a news encoder and\na user encoder. In both encoders we apply multi-\nhead self-attentions to learn contextual word and\nnews representations by modeling the interactions\nbetween words and news. In addition, we use\nadditive attentions to select important words and\nnews to learn more informative news and user rep-\nresentations. Extensive experiments validate the\neffectiveness and efﬁciency of our approach.\nIn our future work, we will try to improve\nour approach in the following potential directions.\nFirst, in our framework we do not consider the po-\nsitional information of words and news, but they\nmay be useful for learning more accurate news and\nuser representations. We will explore position en-\ncoding techniques to incorporate the word position\nand the time-stamps of news clicks to further en-\nhance our approach. Second, we will explore how\nto effectively incorporate multiple kinds of news\ninformation in our framework, especially long se-\nquences such as news body, which may challenge\nthe efﬁciency of typical self-attention networks.\nAcknowledgments\nThe authors would like to thank Microsoft News\nfor providing technical support and data in the\nexperiments, and Jiun-Hung Chen (Microsoft\nNews) and Ying Qiao (Microsoft News) for their\nsupport and discussions. This work was sup-\nported by the National Key Research and De-\nvelopment Program of China under Grant num-\nber 2018YFC1604002, the National Natural Sci-\nence Foundation of China under Grant numbers\nU1836204, U1705261, U1636113, U1536201,\nand U1536207.",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 4,
                },
                "type": "Document",
            },
            {
                "page_content": "References\nMingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang,\nZheng Liu, and Xing Xie. 2019. Neural news rec-\nommendation with long-and short-term user repre-\nsentations. In ACL, pages 336–345.\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal\nShaked, Tushar Chandra, Hrishi Aradhye, Glen An-\nderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.\n2016. Wide & deep learning for recommender sys-\ntems. In DLRS , pages 7–10.\nAbhinandan S Das, Mayur Datar, Ashutosh Garg, and\nShyam Rajaram. 2007. Google news personal-\nization: scalable online collaborative ﬁltering. In\nWWW , pages 271–280. ACM.\nHuifeng Guo, Ruiming Tang, Yunming Ye, Zhen-\nguo Li, and Xiuqiang He. 2017. Deepfm: a\nfactorization-machine based neural network for ctr\nprediction. In AAAI , pages 1725–1731. AAAI Press.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In CIKM , pages 2333–2338.\nACM.\nWouter IJntema, Frank Goossen, Flavius Frasincar,\nand Frederik Hogenboom. 2010. Ontology-based\nnews recommendation. In Proceedings of the 2010\nEDBT/ICDT Workshops , page 16.\nDhruv Khattar, Vaibhav Kumar, Vasudeva Varma, and\nManish Gupta. 2018. Weave&rec: A word embed-\nding based 3-d convolutional network for news rec-\nommendation. In CIKM , pages 1855–1858.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 .\nVaibhav Kumar, Dhruv Khattar, Shashank Gupta, Man-\nish Gupta, and Vasudeva Varma. 2017. Deep neu-\nral architecture for news recommendation. In CLEF\n(Working Notes) .\nJianxun Lian, Fuzheng Zhang, Xing Xie, and\nGuangzhong Sun. 2018. Towards better represen-\ntation learning for personalized news recommenda-\ntion: a multi-channel deep fusion approach. In IJ-\nCAI, pages 3805–3811.Shumpei Okura, Yukihiro Tagami, Shingo Ono, and\nAkira Tajima. 2017. Embedding-based news rec-\nommendation for millions of users. In KDD , pages\n1933–1942. ACM.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP , pages 1532–1543.\nOwen Phelan, Kevin McCarthy, Mike Bennett, and\nBarry Smyth. 2011. Terms of a feather: Content-\nbased news recommendation and discovery using\ntwitter. In ECIR , pages 448–459. Springer.\nSteffen Rendle. 2012. Factorization machines with\nlibfm. TIST , 3(3):57.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS , pages 5998–6008.\nHongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi\nGuo. 2018. Dkn: Deep knowledge-aware network\nfor news recommendation. In WWW , pages 1835–\n1844.\nChuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang\nHuang, Yongfeng Huang, and Xing Xie. 2019a.\nNeural news recommendation with attentive multi-\nview learning. In IJCAI , pages 3863–3869.\nChuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang\nHuang, Yongfeng Huang, and Xing Xie. 2019b.\nNpa: Neural news recommendation with personal-\nized attention. In KDD , pages 2576–2584. ACM.\nChuhan Wu, Fangzhao Wu, Mingxiao An, Yongfeng\nHuang, and Xing Xie. 2019c. Neural news recom-\nmendation with topic-aware news representation. In\nACL, pages 1154–1159.\nChuhan Wu, Fangzhao Wu, Junxin Liu, Sixing Wu,\nYongfeng Huang, and Xing Xie. 2018. Detect-\ning tweets mentioning drug name and adverse drug\nreaction with hierarchical tweet representation and\nmulti-head self-attention. In SMM4H , pages 34–37.\nQiannan Zhu, Xiaofei Zhou, Zeliang Song, Jianlong\nTan, and Li Guo. 2019. Dan: Deep attention neural\nnetwork for news recommendation. In AAAI , vol-\nume 33, pages 5973–5980.",
                "metadata": {
                    "source": "/var/folders/tj/xw68bt0s3nl2s0b1lhpgbcyr0000gn/T/tmpetavl9dt.pdf",
                    "page": 5,
                },
                "type": "Document",
            },
        ]
